{
 "metadata": {
  "name": "",
  "signature": "sha256:19c712b4626d31ad786b5e70890c4d592e6952975a03f8fb317d4de9308b6f32"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import xml.etree.ElementTree as ET\n",
      "tree = ET.parse('WeatherForecast.asmx')\n",
      "root = tree.getroot()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"mainchildren.txt\", \"w\")\n",
      "for child in root:\n",
      "   f.write(child.tag)\n",
      "   f.write(str(child.attrib))\n",
      "   f.write(\"\\n\")\n",
      " \n",
      "f.close()\n",
      "#print file(\"mainchildren.txt\").read() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print (root.tag)       #{http://schemas.xmlsoap.org/wsdl/}definitions\n",
      "print(root.attrib)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "{http://schemas.xmlsoap.org/wsdl/}definitions\n",
        "{'targetNamespace': 'http://www.webservicex.net'}\n"
       ]
      }
     ],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#[elem.attrib for elem in root.iter()]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = open(\"children.txt\", \"w\")\n",
      "for elem in root.iter():\n",
      "   f.write(elem.tag)\n",
      "   f.write(str(elem.attrib))\n",
      "   f.write(\"\\n\")\n",
      " \n",
      "f.close()\n",
      "#print file(\"children.txt\").read() "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "xyz=[]\n",
      "#root.findall(\"./\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 66
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#import xml.etree.ElementTree as ET\n",
      "#tree = ET.parse('globalweather.asmx')\n",
      "#root = tree.getroot()\n",
      "'''\n",
      "for child in root:\n",
      "   #print child.tag\n",
      "    \n",
      "#from xml.etree.ElementTree import XMLParser\n",
      " class MaxDepth:                     # The target object of the parser\n",
      "    maxDepth = 0\n",
      "    self=root\n",
      "    depth = 0\n",
      "    def start(self, tag, attrib):   # Called for each opening tag.\n",
      "        self.depth += 1\n",
      "        if self.depth > self.maxDepth:\n",
      "             self.maxDepth = self.depth\n",
      "    def end(self, tag):             # Called for each closing tag.\n",
      "         self.depth -= 1\n",
      "    def data(self, data):\n",
      "        pass            # We do not need to do anything with data.\n",
      "    def close(self):    # Called when all data has been parsed.\n",
      "        return self.maxDepth\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 67,
       "text": [
        "'\\nfor child in root:\\n   #print child.tag\\n    \\n#from xml.etree.ElementTree import XMLParser\\n class MaxDepth:                     # The target object of the parser\\n    maxDepth = 0\\n    self=root\\n    depth = 0\\n    def start(self, tag, attrib):   # Called for each opening tag.\\n        self.depth += 1\\n        if self.depth > self.maxDepth:\\n             self.maxDepth = self.depth\\n    def end(self, tag):             # Called for each closing tag.\\n         self.depth -= 1\\n    def data(self, data):\\n        pass            # We do not need to do anything with data.\\n    def close(self):    # Called when all data has been parsed.\\n        return self.maxDepth\\n'"
       ]
      }
     ],
     "prompt_number": 67
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "FC = open(\"WeatherForecast.asmx\", \"r\")\n",
      "FCC = open(\"children.txt\", \"r\")\n",
      "FF = open(\"content.txt\", \"w+b\")\n",
      "FM = open(\"message.txt\", \"w+b\")\n",
      "FT = open(\"types.txt\", \"w+b\")\n",
      "\n",
      "\n",
      "elements=FC.read()#needed for newline(message)\n",
      "#print elements\n",
      "elementsc=FCC.read()#for content and message(wsdl)\n",
      "#print elementsc\n",
      "import re\n",
      "import pandas as pd\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "string=re.sub(r\"{http.*/}\",\"\",elementsc,flags=re.I)\n",
      "#print string\n",
      "string2=re.sub(r\"{http[^{}]*}\",\"\",string,flags=re.I)\n",
      "#print string2\n",
      "FF.write(string2)\n",
      "#ff.write(elementsc)\n",
      "\n",
      "\n",
      "newline=re.sub(r'\\n',\"\",elements,flags=re.I)\n",
      "#print newline;\n",
      "newline1=re.sub(r'\\n',\"\",string2,flags=re.I)\n",
      "#print newline1;\n",
      "\n",
      "'''matchObj=re.findall(r\"message{.*\",string,flags=re.I)\n",
      "if matchObj:\n",
      "   print matchObj\n",
      "   for item in matchObj:\n",
      "     fm.write(\"%s\\n\" % item)\n",
      "else:\n",
      "   print \"No match for message\"'''\n",
      "    \n",
      "\n",
      "matchObj=re.findall(r\"message{(.*?)}\",string2,flags=re.I)\n",
      "if matchObj:\n",
      "   #print matchObj\n",
      "   for item in matchObj:\n",
      "         FM.write(\"%s\\n\" % item)\n",
      "   matchObj2=re.findall(r\"part{(.*?)}\",string2,flags=re.I)\n",
      "   if matchObj2:\n",
      "       #print matchObj2\n",
      "       for item in matchObj:\n",
      "         FM.write(\"%s\\n\" % item)\n",
      "else:\n",
      "   matchObj=re.findall(r\"<message(.*?)[^>](.*?)</message>\",newline,flags=re.I)\n",
      "   if matchObj:\n",
      "       #print matchObj\n",
      "       for item in matchObj:\n",
      "        for items in item:\n",
      "         FM.write(\"%s\\n\" % items)\n",
      "   else:\n",
      "       print \"No match at all for message\"\n",
      "\n",
      "                \n",
      "matchObj=re.findall(r\"complexType{(.*?)}\",string2,flags=re.I)\n",
      "if matchObj:\n",
      "   print matchObj\n",
      "   for item in matchObj:\n",
      "         FT.write(\"%s\\n\" % item)\n",
      "   matchObj2=re.findall(r\"element{(.*?)}\",string2,flags=re.I)\n",
      "   if matchObj2:\n",
      "       print matchObj2\n",
      "       for item in matchObj:\n",
      "         FT.write(\"%s\\n\" % item)\n",
      "else:\n",
      "   print \"No match at all for type\"\n",
      "\n",
      "FC.close()\n",
      "FCC.close()\n",
      "FT.close()\n",
      "FM.close()\n",
      "FF.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "['', '', \"'name': 'WeatherForecasts'\", \"'name': 'ArrayOfWeatherData'\", \"'name': 'WeatherData'\", '', '']\n",
        "[\"'name': 'GetWeatherByZipCode'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'ZipCode', 'minOccurs': '0'\", \"'name': 'GetWeatherByZipCodeResponse'\", \"'maxOccurs': '1', 'type': 'tns:WeatherForecasts', 'name': 'GetWeatherByZipCodeResult', 'minOccurs': '1'\", \"'maxOccurs': '1', 'type': 's:float', 'name': 'Latitude', 'minOccurs': '1'\", \"'maxOccurs': '1', 'type': 's:float', 'name': 'Longitude', 'minOccurs': '1'\", \"'maxOccurs': '1', 'type': 's:float', 'name': 'AllocationFactor', 'minOccurs': '1'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'FipsCode', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'PlaceName', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'StateCode', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'Status', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 'tns:ArrayOfWeatherData', 'name': 'Details', 'minOccurs': '0'\", \"'maxOccurs': 'unbounded', 'type': 'tns:WeatherData', 'name': 'WeatherData', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'Day', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'WeatherImage', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'MaxTemperatureF', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'MinTemperatureF', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'MaxTemperatureC', 'minOccurs': '0'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'MinTemperatureC', 'minOccurs': '0'\", \"'name': 'GetWeatherByPlaceName'\", \"'maxOccurs': '1', 'type': 's:string', 'name': 'PlaceName', 'minOccurs': '0'\", \"'name': 'GetWeatherByPlaceNameResponse'\", \"'maxOccurs': '1', 'type': 'tns:WeatherForecasts', 'name': 'GetWeatherByPlaceNameResult', 'minOccurs': '1'\", \"'type': 'tns:WeatherForecasts', 'name': 'WeatherForecasts'\"]\n"
       ]
      }
     ],
     "prompt_number": 68
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#from SOAPpy import WSDL     \n",
      "from suds.client import Client\n",
      "import re\n",
      "import xml.etree.ElementTree as ET\n",
      "\n",
      "fp = open(\"portType.txt\", \"w+b\")\n",
      "fs = open(\"services.txt\", \"w+b\")\n",
      "fs1 = open(\"sudsps.txt\", \"w\")\n",
      "\n",
      "url = 'file:///home/swathi/WeatherForecast.asmx'\n",
      "c = Client(url)\n",
      "#print c;\n",
      "fs1.write(\"%s\\n\" % c)\n",
      "\n",
      "fs1.close()\n",
      "\n",
      "fs2 = open(\"sudsps.txt\", \"r\")\n",
      "sudselements=fs2.read()\n",
      "#print sudselements\n",
      "\n",
      "newline2=re.sub(r'\\n',\"\",sudselements,flags=re.I)\n",
      "#print newline1;\n",
      "\n",
      "matchObj=re.findall(r\"Ports(.*)\",newline2,flags=re.I)\n",
      "if matchObj:\n",
      "   #print matchObj\n",
      "   for item in matchObj:\n",
      "        for items in item:\n",
      "         fp.write(items)\n",
      "else:\n",
      "   print \"No match for ports\"\n",
      "    \n",
      "matchObj=re.findall(r\"Service(.*)\\((.*)\\)\",sudselements,flags=re.I)\n",
      "if matchObj:\n",
      "   #print matchObj\n",
      "   for item in matchObj:\n",
      "        for items in item:\n",
      "         fs.write(\"%s\\n\" % items)\n",
      "else:\n",
      "   print \"No match for service name\"\n",
      "\n",
      "\n",
      "fp.close()\n",
      "fs2.close()\n",
      "fs.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "stemmed_filem = open(\"M.txt\",\"w+b\")\n",
      "stemmed_files = open(\"S.txt\",\"w+b\")\n",
      "stemmed_filep = open(\"P.txt\",\"w+b\")\n",
      "stemmed_filec = open(\"C.txt\",\"w+b\")\n",
      "stemmed_filet = open(\"T.txt\", \"w+b\")\n",
      "\n",
      "import re\n",
      "from nltk import PorterStemmer\n",
      "with open('portType.txt','r') as f:\n",
      "    for line in f:\n",
      "      for word in line.split():\n",
      "       #print(word)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',word, flags=re.I)\n",
      "       #print all_alpha\n",
      "       stemmer=PorterStemmer()\n",
      "       new=stemmer.stem(all_alpha)\n",
      "       #print new\n",
      "       stemmed_filep.write(new)\n",
      "\n",
      "with open('message.txt','r') as f:\n",
      "    for line in f:\n",
      "     for word in line.split():\n",
      "\n",
      "       #print(word)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',word, flags=re.I)\n",
      "       #print all_alpha\n",
      "       stemmer=PorterStemmer()\n",
      "       new=stemmer.stem(all_alpha)\n",
      "       stemmed_filem.write(new)\n",
      "\n",
      "with open('services.txt','r') as f:\n",
      "    for line in f:\n",
      "     for word in line.split():\n",
      "\n",
      "       #print(word)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',word, flags=re.I)\n",
      "       #print all_alpha\n",
      "       stemmer=PorterStemmer()\n",
      "       stemmer.stem(all_alpha)\n",
      "       stemmed_files.write(all_alpha)\n",
      "\n",
      "with open('types.txt','r') as f:\n",
      "    for line in f:\n",
      "     for word in line.split():\n",
      "\n",
      "       #print(word)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',word, flags=re.I)\n",
      "       #print all_alpha\n",
      "       stemmer=PorterStemmer()\n",
      "       stemmer.stem(all_alpha)\n",
      "       stemmed_filet.write(all_alpha)\n",
      "\n",
      "with open('content.txt','r') as f:\n",
      "    for line in f:\n",
      "     for word in line.split():\n",
      "\n",
      "       #print(word)\n",
      "       all_alpha = re.sub(r'^\\'https?:\\/\\/.*[\\r\\n]*\\'','',word, flags=re.I)\n",
      "       all_alpha = re.sub(r'^\\{http?:\\/\\/.*[\\r\\n]*\\\\}','',all_alpha, flags=re.I)\n",
      "       all_alpha = re.sub('[^a-zA-Z_]',' ',all_alpha, flags=re.I)\n",
      "       #print all_alpha\n",
      "       for word2 in all_alpha.split():\n",
      "           #print word2\n",
      "           stemmer=PorterStemmer()\n",
      "           new=stemmer.stem(word2)\n",
      "           #print new\n",
      "           stemmed_filec.write(new)\n",
      "           stemmed_filec.write(' ')\n",
      "\n",
      "stemmed_filec.close()\n",
      "stemmed_filem.close()\n",
      "stemmed_files.close()\n",
      "stemmed_filet.close()\n",
      "stemmed_filep.close()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk import PorterStemmer\n",
      "stemmer=PorterStemmer()\n",
      "word='execution'\n",
      "stemmer.stem(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "u'execut'"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''import difflib as dl\n",
      "\n",
      "a = file('WS/1/C.txt').read()\n",
      "b = file('WS/2/C.txt').read()\n",
      "\n",
      "sim = dl.get_close_matches\n",
      "\n",
      "s = 0\n",
      "wa = a.split()\n",
      "wb = b.split()\n",
      "\n",
      "for i in wa:\n",
      "    if sim(i, wb):\n",
      "        s += 1\n",
      "\n",
      "n = float(s) / float(len(wa))\n",
      "print '%d%% similarity' % int(n * 100)'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 151,
       "text": [
        "\"import difflib as dl\\n\\na = file('WS/1/C.txt').read()\\nb = file('WS/2/C.txt').read()\\n\\nsim = dl.get_close_matches\\n\\ns = 0\\nwa = a.split()\\nwb = b.split()\\n\\nfor i in wa:\\n    if sim(i, wb):\\n        s += 1\\n\\nn = float(s) / float(len(wa))\\nprint '%d%% similarity' % int(n * 100)\""
       ]
      }
     ],
     "prompt_number": 151
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "N=0\n",
      "F = open(\"WS/N.txt\",\"w+b\")\n",
      "\n",
      "with open(\"WS/6/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "     #    with open(\"WS/N.txt\",\"r\") as f2:\n",
      "     #       for line2 in f:\n",
      "      #          for word2 in line2.split():\n",
      "     #             if word == word2\n",
      "     #               Na = f2.read(word2)\n",
      "      #              F.write(str(Na+1)+'\\n')\n",
      "    #                break\n",
      "    #              else\n",
      "    #               continue\n",
      "    #            if(word!=word2)\n",
      "    #                Na=1\n",
      "   #                 F.write(word)#if not present, add the word with count equal to 1\n",
      "   #                 F.write(\" \")\n",
      "   #                 F.write(str(Na)+'\\n')                   \n",
      "          #F.write(all_alpha)'''\n",
      "        \n",
      "with open(\"WS/5/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "\n",
      "with open(\"WS/0/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "                \n",
      "with open(\"WS/1/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "          \n",
      "with open(\"WS/3/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "                \n",
      "with open(\"WS/7/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract each word from new doc\n",
      "          N=N+1\n",
      "          if len(word)!=1:\n",
      "             F.write(word+'\\n')\n",
      "F.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "F = open(\"WS/C.txt\",\"w+b\")\n",
      "LamAv=0\n",
      "print N\n",
      "with open(\"WS/2/C.txt\",\"r\") as f:\n",
      "    for line in f:\n",
      "        for word in line.split():#extract word\n",
      "            if(len(word)>1):\n",
      "              a=word\n",
      "              Fa=1\n",
      "              Nw=1\n",
      "              with open(\"WS/2/C.txt\",\"r\") as f2:\n",
      "               for line2 in f2:\n",
      "                for word2 in line2.split():\n",
      "                    if (a==word2):\n",
      "                     Fa=Fa+1\n",
      "              with open(\"WS/3/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "              with open(\"WS/5/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "              with open(\"WS/7/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "              with open(\"WS/1/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "              with open(\"WS/0/C.txt\",\"r\") as f3:\n",
      "               flag=0\n",
      "               for line3 in f3:\n",
      "                for word3 in line3.split():\n",
      "                    if (a==word3):\n",
      "                     Fa=Fa+1\n",
      "                     flag=1\n",
      "                if flag==1:\n",
      "                    Nw=Nw+1\n",
      "\n",
      "              #print Fa\n",
      "              #print Nw\n",
      "              La=float(Fa*100/N)\n",
      "              #print La\n",
      "              temp=(np.random.poisson(0,La))\n",
      "              Na = N*(1-temp)\n",
      "              Lam=Na/Nw\n",
      "              Lamthr= Nw/5#no of docs observed\n",
      "              if (Lam.all() > Lamthr):\n",
      "                    F.write(a+'\\n')\n",
      "                    \n",
      "F.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "24639\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/2/NonDup2.txt\", \"w\")\n",
      "for line in open(\"WS/C.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfile = open(\"WS/2/NonDup.txt\", \"w\")\n",
      "for line in open(\"WS/C.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()\n",
      "\n",
      "#outfile = open(\"WS/3/T1.txt\", \"w\")\n",
      "#for line in open(\"WS/3/T.txt\", \"r\"):\n",
      "#  for word in line.split():\n",
      "#    if(word!=\"name\"):\n",
      "#        outfile.write(word+'\\n')\n",
      "#outfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "\n",
      "outfile = open(\"WS/0/T1.txt\", \"w\")\n",
      "for line in open(\"WS/0/T.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()\n",
      "\n",
      "outfile = open(\"WS/2/T1.txt\", \"w\")\n",
      "for line in open(\"WS/2/T.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/0/T2.txt\", \"w\")\n",
      "for line in open(\"WS/0/T1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/2/T2.txt\", \"w\")\n",
      "for line in open(\"WS/2/T1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "Mt=0\n",
      "F = open(\"WS/0/T2.txt\",\"r\")\n",
      "for line in F:\n",
      "  for word in line.split():\n",
      "   a=word\n",
      "   print word \n",
      "   with open(\"WS/2/T2.txt\",\"r\") as f:\n",
      "    for line2 in f:\n",
      "        for word2 in line2.split():#extract word\n",
      "             if(a==word2) :\n",
      "              Mt=Mt+1\n",
      "print Mt\n",
      "\n",
      "F.close()\n",
      "\n",
      "E1=0\n",
      "E2=0\n",
      "\n",
      "F1 = open(\"WS/0/T2.txt\",\"r\")\n",
      "for line in F1:\n",
      "  for word in line.split():\n",
      "   E1=E1+1\n",
      "F1.close()\n",
      "\n",
      "F2 = open(\"WS/2/T2.txt\",\"r\")\n",
      "for line in F2:\n",
      "  for word in line.split():\n",
      "   E2=E2+1\n",
      "F2.close()\n",
      "from decimal import Decimal\n",
      "print E1\n",
      "print E2\n",
      "print Decimal(E1+E2)/2\n",
      "print Decimal(Mt/(Decimal(E1+E2)/2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AuthenticationV\n",
        "RecurringV\n",
        "BillingDetailsV\n",
        "CardExpiryV\n",
        "CardV\n",
        "CCBatchRequestV\n",
        "CCProfileRequest\n",
        "CCProfileResponse\n",
        "CCProfileCardResponse\n",
        "CCAuthRequestV\n",
        "CCPostAuthRequestV\n",
        "CCPaymentRequestV\n",
        "CCCancelRequestV\n",
        "CCAcknowledgeResponseV\n",
        "CCBatchResponseV\n",
        "CCTxnResponseV\n",
        "DetailV\n",
        "SalesTaxV\n",
        "MerchantAccountV\n",
        "SdkV\n",
        "ShippingDetailsV\n",
        "TDSResponseV\n",
        "CCAuthenticateRequestV\n",
        "CCStoredDataRequestV\n",
        "CCEnrollmentLookupRequestV\n",
        "TDSAuthenticateResponseV\n",
        "CCAuthReversalRequestV\n",
        "AddendumResponseV\n",
        "AccordDV\n",
        "LocationV\n",
        "RiskResponseV\n",
        "CCAuthStatusRequestV\n",
        "CCTxnLookupRequestV\n",
        "DateV\n",
        "VisaAdditionalAuthDataV\n",
        "MasterPassV\n",
        "ping\n",
        "pingResponse\n",
        "ccPayment\n",
        "ccPaymentResponse\n",
        "ccPurchase\n",
        "ccPurchaseResponse\n",
        "ccTDSAuthenticate\n",
        "ccTDSAuthenticateResponse\n",
        "ccAuthenticate\n",
        "ccAuthenticateResponse\n",
        "ccSettlement\n",
        "ccSettlementResponse\n",
        "ccAuthorize\n",
        "ccAuthorizeResponse\n",
        "ccCancelIndependentCredit\n",
        "ccCancelIndependentCreditResponse\n",
        "ccVerification\n",
        "ccVerificationResponse\n",
        "ccStoredDataPurchase\n",
        "ccStoredDataPurchaseResponse\n",
        "ccCredit\n",
        "ccCreditResponse\n",
        "ccStoredDataAuthorize\n",
        "ccStoredDataAuthorizeResponse\n",
        "ccTDSLookup\n",
        "ccTDSLookupResponse\n",
        "ccCancelCredit\n",
        "ccCancelCreditResponse\n",
        "ccCancelSettle\n",
        "ccCancelSettleResponse\n",
        "ccCancelPayment\n",
        "ccCancelPaymentResponse\n",
        "updateAuthorizationStatus\n",
        "updateAuthorizationStatusResponse\n",
        "ccIndependentCredit\n",
        "ccIndependentCreditResponse\n",
        "ccTxnLookup\n",
        "ccTxnLookupResponse\n",
        "ccAuthorizeReversal\n",
        "ccAuthorizeReversalResponse\n",
        "0\n",
        "76\n",
        "0\n",
        "38\n",
        "0\n"
       ]
      }
     ],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfile = open(\"WS/0/M1.txt\", \"w\")\n",
      "for line in open(\"WS/0/M.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()\n",
      "\n",
      "outfile = open(\"WS/2/M1.txt\", \"w\")\n",
      "for line in open(\"WS/2/M.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()                \n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/0/M2.txt\", \"w\")\n",
      "for line in open(\"WS/0/M1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/2/M2.txt\", \"w\")\n",
      "for line in open(\"WS/2/M1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "Mt=0\n",
      "F = open(\"WS/0/M2.txt\",\"r\")\n",
      "for line in F:\n",
      "  for word in line.split():\n",
      "   a=word\n",
      "  # print word \n",
      "   with open(\"WS/2/M2.txt\",\"r\") as f:\n",
      "    for line2 in f:\n",
      "        for word2 in line2.split():#extract word\n",
      "             if(a==word2) :\n",
      "              Mt=Mt+1\n",
      "print \"Matching words -\",Mt\n",
      "F.close()\n",
      "\n",
      "E1=0\n",
      "E2=0\n",
      "\n",
      "F1 = open(\"WS/0/M2.txt\",\"r\")\n",
      "for line in F1:\n",
      "  for word in line.split():\n",
      "   E1=E1+1\n",
      "F1.close()\n",
      "\n",
      "F2 = open(\"WS/2/M2.txt\",\"r\")\n",
      "for line in F2:\n",
      "  for word in line.split():\n",
      "   E2=E2+1\n",
      "F2.close()\n",
      "\n",
      "print \"Number of elements in first document - \",E1\n",
      "print \"Number of elements in second document - \",E2\n",
      "from decimal import Decimal\n",
      "#print Decimal(E1+E2)/2\n",
      "print \"Match between the Messages of two documents - \",Decimal(Mt/(Decimal(E1+E2)/2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Matching words - 0\n",
        "Number of elements in first document -  40\n",
        "Number of elements in second document -  2\n",
        "Match between the Messages of two documents -  0\n"
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "outfile = open(\"WS/0/P1.txt\", \"w\")\n",
      "for line in open(\"WS/0/P.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()\n",
      "\n",
      "outfile = open(\"WS/2/P1.txt\", \"w\")\n",
      "for line in open(\"WS/2/P.txt\", \"r\"):\n",
      "  for word in line.split():\n",
      "    if(word!=\"name\"):\n",
      "        outfile.write(word+'\\n')\n",
      "outfile.close()                \n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/0/P2.txt\", \"w\")\n",
      "for line in open(\"WS/0/P1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "lines_seen = set() # holds lines already seen\n",
      "outfile = open(\"WS/2/P2.txt\", \"w\")\n",
      "for line in open(\"WS/2/P1.txt\", \"r\"):\n",
      "    if line not in lines_seen: # not a duplicate\n",
      "        outfile.write(line)\n",
      "        lines_seen.add(line)\n",
      "outfile.close()\n",
      "\n",
      "Mt=0\n",
      "F = open(\"WS/0/P2.txt\",\"r\")\n",
      "for line in F:\n",
      "  for word in line.split():\n",
      "   a=word\n",
      "   #print word \n",
      "   with open(\"WS/2/P2.txt\",\"r\") as f:\n",
      "    for line2 in f:\n",
      "        for word2 in line2.split():#extract word\n",
      "             if(a==word2) :\n",
      "              Mt=Mt+1\n",
      "print \"Matching words -\",Mt\n",
      "F.close()\n",
      "\n",
      "E1=0\n",
      "E2=0\n",
      "\n",
      "F1 = open(\"WS/0/P2.txt\",\"r\")\n",
      "for line in F1:\n",
      "  for word in line.split():\n",
      "   E1=E1+1\n",
      "F1.close()\n",
      "\n",
      "F2 = open(\"WS/2/P2.txt\",\"r\")\n",
      "for line in F2:\n",
      "  for word in line.split():\n",
      "   E2=E2+1\n",
      "F2.close()\n",
      "\n",
      "print \"Number of elements in first document - \",E1\n",
      "print \"Number of elements in second document - \",E2\n",
      "from decimal import Decimal\n",
      "#print Decimal(E1+E2)/2\n",
      "print \"Match between the Ports of two documents - \",Decimal(Mt/(Decimal(E1+E2)/2))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Matching words - 2\n",
        "Number of elements in first document -  108\n",
        "Number of elements in second document -  6\n",
        "Match between the Ports of two documents -  0.03508771929824561403508771930\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "F = open(\"Practice.txt\",\"w+b\")\n",
      "F.write(\"hello\")\n",
      "N=2\n",
      "F.write(\" \")\n",
      "F.write(str(N)+'\\n')\n",
      "F.close()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "word=\"hello\"\n",
      "word2=\"hello\"\n",
      "\n",
      "len(word)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 16,
       "text": [
        "5"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "s = np.random.poisson(0, 3)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 17
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn import datasets\n",
      "\n",
      "np.random.seed(5)\n",
      "\n",
      "centers = [[1, 1], [-1, -1], [1, -1]]\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "estimators = {'k_means_iris_3': KMeans(n_clusters=3),\n",
      "              'k_means_iris_8': KMeans(n_clusters=8),\n",
      "              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n",
      "                                              init='random')}\n",
      "\n",
      "\n",
      "fignum = 1\n",
      "for name, est in estimators.items():\n",
      "    fig = plt.figure(fignum, figsize=(4, 3))\n",
      "    plt.clf()\n",
      "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "    plt.cla()\n",
      "    est.fit(X)\n",
      "    labels = est.labels_\n",
      "\n",
      "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n",
      "\n",
      "    ax.w_xaxis.set_ticklabels([])\n",
      "    ax.w_yaxis.set_ticklabels([])\n",
      "    ax.w_zaxis.set_ticklabels([])\n",
      "    ax.set_xlabel('Petal width')\n",
      "    ax.set_ylabel('Sepal length')\n",
      "    ax.set_zlabel('Petal length')\n",
      "    fignum = fignum + 1\n",
      "\n",
      "# Plot the ground truth\n",
      "fig = plt.figure(fignum, figsize=(4, 3))\n",
      "plt.clf()\n",
      "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "plt.cla()\n",
      "\n",
      "for name, label in [('Setosa', 0),\n",
      "                    ('Versicolour', 1),\n",
      "                    ('Virginica', 2)]:\n",
      "    ax.text3D(X[y == label, 3].mean(),\n",
      "              X[y == label, 0].mean() + 1.5,\n",
      "              X[y == label, 2].mean(), name,\n",
      "              horizontalalignment='center',\n",
      "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
      "# Reorder the labels to have colors matching the cluster results\n",
      "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
      "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n",
      "\n",
      "ax.w_xaxis.set_ticklabels([])\n",
      "ax.w_yaxis.set_ticklabels([])\n",
      "ax.w_zaxis.set_ticklabels([])\n",
      "ax.set_xlabel('Petal width')\n",
      "ax.set_ylabel('Sepal length')\n",
      "ax.set_zlabel('Petal length')\n",
      "plt.show()\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/swathi/anaconda/lib/python2.7/site-packages/mpl_toolkits/mplot3d/axes3d.py:1094: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
        "  if self.button_pressed in self._rotate_btn:\n"
       ]
      }
     ],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn import datasets\n",
      "\n",
      "np.random.seed(5)\n",
      "\n",
      "centers = [[1, 1], [-1, -1], [1, -1]]\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "estimators = {'k_means_iris_3': KMeans(n_clusters=3),\n",
      "              'k_means_iris_8': KMeans(n_clusters=8),\n",
      "              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n",
      "                                              init='random')}\n",
      "\n",
      "\n",
      "fignum = 1\n",
      "for name, est in estimators.items():\n",
      "    fig = plt.figure(fignum, figsize=(4, 3))\n",
      "    plt.clf()\n",
      "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "    plt.cla()\n",
      "    est.fit(X)\n",
      "    labels = est.labels_\n",
      "\n",
      "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n",
      "\n",
      "    ax.w_xaxis.set_ticklabels([])\n",
      "    ax.w_yaxis.set_ticklabels([])\n",
      "    ax.w_zaxis.set_ticklabels([])\n",
      "    ax.set_xlabel('Petal width')\n",
      "    ax.set_ylabel('Sepal length')\n",
      "    ax.set_zlabel('Petal length')\n",
      "    fignum = fignum + 1\n",
      "\n",
      "# Plot the ground truth\n",
      "fig = plt.figure(fignum, figsize=(4, 3))\n",
      "plt.clf()\n",
      "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "plt.cla()\n",
      "\n",
      "for name, label in [('Setosa', 0),\n",
      "                    ('Versicolour', 1),\n",
      "                    ('Virginica', 2)]:\n",
      "    ax.text3D(X[y == label, 3].mean(),\n",
      "              X[y == label, 0].mean() + 1.5,\n",
      "              X[y == label, 2].mean(), name,\n",
      "              horizontalalignment='center',\n",
      "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
      "# Reorder the labels to have colors matching the cluster results\n",
      "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
      "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n",
      "\n",
      "ax.w_xaxis.set_ticklabels([])\n",
      "ax.w_yaxis.set_ticklabels([])\n",
      "ax.w_zaxis.set_ticklabels([])\n",
      "ax.set_xlabel('Petal width')\n",
      "ax.set_ylabel('Sepal length')\n",
      "ax.set_zlabel('Petal length')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import cluster\n",
      "\n",
      "n_clusters = 5\n",
      "np.random.seed(0)\n",
      "\n",
      "try:\n",
      "    lena = sp.lena()\n",
      "except AttributeError:\n",
      "    # Newer versions of scipy have lena in misc\n",
      "    from scipy import misc\n",
      "    lena = misc.lena()\n",
      "X = lena.reshape((-1, 1))  # We need an (n_sample, n_feature) array\n",
      "k_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)\n",
      "k_means.fit(X)\n",
      "values = k_means.cluster_centers_.squeeze()\n",
      "labels = k_means.labels_\n",
      "\n",
      "# create an array from labels and values\n",
      "lena_compressed = np.choose(labels, values)\n",
      "lena_compressed.shape = lena.shape\n",
      "\n",
      "vmin = lena.min()\n",
      "vmax = lena.max()\n",
      "\n",
      "# original lena\n",
      "plt.figure(1, figsize=(3, 2.2))\n",
      "plt.imshow(lena, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n",
      "\n",
      "# compressed lena\n",
      "plt.figure(2, figsize=(3, 2.2))\n",
      "plt.imshow(lena_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
      "\n",
      "# equal bins lena\n",
      "regular_values = np.linspace(0, 256, n_clusters + 1)\n",
      "regular_labels = np.searchsorted(regular_values, lena) - 1\n",
      "regular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\n",
      "regular_lena = np.choose(regular_labels.ravel(), regular_values)\n",
      "regular_lena.shape = lena.shape\n",
      "plt.figure(3, figsize=(3, 2.2))\n",
      "plt.imshow(regular_lena, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
      "\n",
      "# histogram\n",
      "plt.figure(4, figsize=(3, 2.2))\n",
      "plt.clf()\n",
      "plt.axes([.01, .01, .98, .98])\n",
      "plt.hist(X, bins=256, color='.5', edgecolor='.5')\n",
      "plt.yticks(())\n",
      "plt.xticks(regular_values)\n",
      "values = np.sort(values)\n",
      "for center_1, center_2 in zip(values[:-1], values[1:]):\n",
      "    plt.axvline(.5 * (center_1 + center_2), color='b')\n",
      "\n",
      "for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n",
      "    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import scipy as sp\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "from sklearn import cluster\n",
      "\n",
      "n_clusters = 5\n",
      "np.random.seed(0)\n",
      "\n",
      "try:\n",
      "    lena = sp.lena()\n",
      "except AttributeError:\n",
      "    # Newer versions of scipy have lena in misc\n",
      "    from scipy import misc\n",
      "    lena = misc.lena()\n",
      "X = lena.reshape((-1, 1))  # We need an (n_sample, n_feature) array\n",
      "k_means = cluster.KMeans(n_clusters=n_clusters, n_init=4)\n",
      "k_means.fit(X)\n",
      "values = k_means.cluster_centers_.squeeze()\n",
      "labels = k_means.labels_\n",
      "\n",
      "# create an array from labels and values\n",
      "lena_compressed = np.choose(labels, values)\n",
      "lena_compressed.shape = lena.shape\n",
      "\n",
      "vmin = lena.min()\n",
      "vmax = lena.max()\n",
      "\n",
      "# original lena\n",
      "plt.figure(1, figsize=(3, 2.2))\n",
      "plt.imshow(lena, cmap=plt.cm.gray, vmin=vmin, vmax=256)\n",
      "\n",
      "# compressed lena\n",
      "plt.figure(2, figsize=(3, 2.2))\n",
      "plt.imshow(lena_compressed, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
      "\n",
      "# equal bins lena\n",
      "regular_values = np.linspace(0, 256, n_clusters + 1)\n",
      "regular_labels = np.searchsorted(regular_values, lena) - 1\n",
      "regular_values = .5 * (regular_values[1:] + regular_values[:-1])  # mean\n",
      "regular_lena = np.choose(regular_labels.ravel(), regular_values)\n",
      "regular_lena.shape = lena.shape\n",
      "plt.figure(3, figsize=(3, 2.2))\n",
      "plt.imshow(regular_lena, cmap=plt.cm.gray, vmin=vmin, vmax=vmax)\n",
      "\n",
      "# histogram\n",
      "plt.figure(4, figsize=(3, 2.2))\n",
      "plt.clf()\n",
      "plt.axes([.01, .01, .98, .98])\n",
      "plt.hist(X, bins=256, color='.5', edgecolor='.5')\n",
      "plt.yticks(())\n",
      "plt.xticks(regular_values)\n",
      "values = np.sort(values)\n",
      "for center_1, center_2 in zip(values[:-1], values[1:]):\n",
      "    plt.axvline(.5 * (center_1 + center_2), color='b')\n",
      "\n",
      "for center_1, center_2 in zip(regular_values[:-1], regular_values[1:]):\n",
      "    plt.axvline(.5 * (center_1 + center_2), color='b', linestyle='--')\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 36
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "import matplotlib.pyplot as plt\n",
      "from mpl_toolkits.mplot3d import Axes3D\n",
      "\n",
      "\n",
      "from sklearn.cluster import KMeans\n",
      "from sklearn import datasets\n",
      "\n",
      "np.random.seed(5)\n",
      "\n",
      "centers = [[1, 1], [-1, -1], [1, -1]]\n",
      "iris = datasets.load_iris()\n",
      "X = iris.data\n",
      "y = iris.target\n",
      "\n",
      "estimators = {'k_means_iris_3': KMeans(n_clusters=3),\n",
      "              'k_means_iris_8': KMeans(n_clusters=8),\n",
      "              'k_means_iris_bad_init': KMeans(n_clusters=3, n_init=1,\n",
      "                                              init='random')}\n",
      "\n",
      "\n",
      "fignum = 1\n",
      "for name, est in estimators.items():\n",
      "    fig = plt.figure(fignum, figsize=(4, 3))\n",
      "    plt.clf()\n",
      "    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "    plt.cla()\n",
      "    est.fit(X)\n",
      "    labels = est.labels_\n",
      "\n",
      "    ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=labels.astype(np.float))\n",
      "\n",
      "    ax.w_xaxis.set_ticklabels([])\n",
      "    ax.w_yaxis.set_ticklabels([])\n",
      "    ax.w_zaxis.set_ticklabels([])\n",
      "    ax.set_xlabel('Petal width')\n",
      "    ax.set_ylabel('Sepal length')\n",
      "    ax.set_zlabel('Petal length')\n",
      "    fignum = fignum + 1\n",
      "\n",
      "# Plot the ground truth\n",
      "fig = plt.figure(fignum, figsize=(4, 3))\n",
      "plt.clf()\n",
      "ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n",
      "\n",
      "plt.cla()\n",
      "\n",
      "for name, label in [('Setosa', 0),\n",
      "                    ('Versicolour', 1),\n",
      "                    ('Virginica', 2)]:\n",
      "    ax.text3D(X[y == label, 3].mean(),\n",
      "              X[y == label, 0].mean() + 1.5,\n",
      "              X[y == label, 2].mean(), name,\n",
      "              horizontalalignment='center',\n",
      "              bbox=dict(alpha=.5, edgecolor='w', facecolor='w'))\n",
      "# Reorder the labels to have colors matching the cluster results\n",
      "y = np.choose(y, [1, 2, 0]).astype(np.float)\n",
      "ax.scatter(X[:, 3], X[:, 0], X[:, 2], c=y)\n",
      "\n",
      "ax.w_xaxis.set_ticklabels([])\n",
      "ax.w_yaxis.set_ticklabels([])\n",
      "ax.w_zaxis.set_ticklabels([])\n",
      "ax.set_xlabel('Petal width')\n",
      "ax.set_ylabel('Sepal length')\n",
      "ax.set_zlabel('Petal length')\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "/home/swathi/anaconda/lib/python2.7/site-packages/mpl_toolkits/mplot3d/axes3d.py:1094: FutureWarning: comparison to `None` will result in an elementwise object comparison in the future.\n",
        "  if self.button_pressed in self._rotate_btn:\n"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "X = [[10, 20,30,1],[30,40,5,7],[2,4,5,8],[1,2,3,4]]\n",
      "\n",
      "listofwords = ['physics', 'chemistry', 'maths', 'bio'];\n",
      "\n",
      "\n",
      "\n",
      "km = KMeans(n_clusters = 2)\n",
      "km.fit(X)\n",
      "    \n",
      "prediction = km.predict(lst)\n",
      "\n",
      "print('{:<15}\\t{}'.format('Word','Cluster'))\n",
      "for i in range(len(prediction)):\n",
      "    print('{:<15}\\t{}'.format(listofwords[i],prediction[i]+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from __future__ import print_function\n",
      "\n",
      "from sklearn.datasets import fetch_20newsgroups\n",
      "from sklearn.decomposition import TruncatedSVD\n",
      "from sklearn.feature_extraction.text import TfidfVectorizer\n",
      "from sklearn.feature_extraction.text import HashingVectorizer\n",
      "from sklearn.feature_extraction.text import TfidfTransformer\n",
      "from sklearn.pipeline import make_pipeline\n",
      "from sklearn.preprocessing import Normalizer\n",
      "from sklearn import metrics\n",
      "\n",
      "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
      "\n",
      "import logging\n",
      "from optparse import OptionParser\n",
      "import sys\n",
      "from time import time\n",
      "\n",
      "import numpy as np\n",
      "\n",
      "\n",
      "# Display progress logs on stdout\n",
      "logging.basicConfig(level=logging.INFO,\n",
      "                    format='%(asctime)s %(levelname)s %(message)s')\n",
      "\n",
      "# parse commandline arguments\n",
      "op = OptionParser()\n",
      "op.add_option(\"--lsa\",\n",
      "              dest=\"n_components\", type=\"int\",\n",
      "              help=\"Preprocess documents with latent semantic analysis.\")\n",
      "op.add_option(\"--no-minibatch\",\n",
      "              action=\"store_false\", dest=\"minibatch\", default=True,\n",
      "              help=\"Use ordinary k-means algorithm (in batch mode).\")\n",
      "op.add_option(\"--no-idf\",\n",
      "              action=\"store_false\", dest=\"use_idf\", default=True,\n",
      "              help=\"Disable Inverse Document Frequency feature weighting.\")\n",
      "op.add_option(\"--use-hashing\",\n",
      "              action=\"store_true\", default=False,\n",
      "              help=\"Use a hashing feature vectorizer\")\n",
      "op.add_option(\"--n-features\", type=int, default=10000,\n",
      "              help=\"Maximum number of features (dimensions)\"\n",
      "                   \" to extract from text.\")\n",
      "op.add_option(\"--verbose\",\n",
      "              action=\"store_true\", dest=\"verbose\", default=False,\n",
      "              help=\"Print progress reports inside k-means algorithm.\")\n",
      "\n",
      "print(__doc__)\n",
      "op.print_help()\n",
      "\n",
      "(opts, args) = op.parse_args()\n",
      "if len(args) > 0:\n",
      "    op.error(\"this script takes no arguments.\")\n",
      "    sys.exit(1)\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Load some categories from the training set\n",
      "categories = [\n",
      "    'alt.atheism',\n",
      "    'talk.religion.misc',\n",
      "    'comp.graphics',\n",
      "    'sci.space',\n",
      "]\n",
      "# Uncomment the following to do the analysis on all the categories\n",
      "#categories = None\n",
      "\n",
      "print(\"Loading 20 newsgroups dataset for categories:\")\n",
      "print(categories)\n",
      "\n",
      "dataset = fetch_20newsgroups(subset='all', categories=categories,\n",
      "                             shuffle=True, random_state=42)\n",
      "\n",
      "print(\"%d documents\" % len(dataset.data))\n",
      "print(\"%d categories\" % len(dataset.target_names))\n",
      "print()\n",
      "\n",
      "labels = dataset.target\n",
      "true_k = np.unique(labels).shape[0]\n",
      "\n",
      "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
      "t0 = time()\n",
      "if opts.use_hashing:\n",
      "    if opts.use_idf:\n",
      "        # Perform an IDF normalization on the output of HashingVectorizer\n",
      "        hasher = HashingVectorizer(n_features=opts.n_features,\n",
      "                                   stop_words='english', non_negative=True,\n",
      "                                   norm=None, binary=False)\n",
      "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
      "    else:\n",
      "        vectorizer = HashingVectorizer(n_features=opts.n_features,\n",
      "                                       stop_words='english',\n",
      "                                       non_negative=False, norm='l2',\n",
      "                                       binary=False)\n",
      "else:\n",
      "    vectorizer = TfidfVectorizer(max_df=0.5, max_features=opts.n_features,\n",
      "                                 min_df=2, stop_words='english',\n",
      "                                 use_idf=opts.use_idf)\n",
      "X = vectorizer.fit_transform(dataset.data)\n",
      "\n",
      "print(\"done in %fs\" % (time() - t0))\n",
      "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
      "print()\n",
      "\n",
      "if opts.n_components:\n",
      "    print(\"Performing dimensionality reduction using LSA\")\n",
      "    t0 = time()\n",
      "    # Vectorizer results are normalized, which makes KMeans behave as\n",
      "    # spherical k-means for better results. Since LSA/SVD results are\n",
      "    # not normalized, we have to redo the normalization.\n",
      "    svd = TruncatedSVD(opts.n_components)\n",
      "    lsa = make_pipeline(svd, Normalizer(copy=False))\n",
      "\n",
      "    X = lsa.fit_transform(X)\n",
      "\n",
      "    print(\"done in %fs\" % (time() - t0))\n",
      "\n",
      "    explained_variance = svd.explained_variance_ratio_.sum()\n",
      "    print(\"Explained variance of the SVD step: {}%\".format(\n",
      "        int(explained_variance * 100)))\n",
      "\n",
      "    print()\n",
      "\n",
      "\n",
      "###############################################################################\n",
      "# Do the actual clustering\n",
      "\n",
      "if opts.minibatch:\n",
      "    km = MiniBatchKMeans(n_clusters=true_k, init='k-means++', n_init=1,\n",
      "                         init_size=1000, batch_size=1000, verbose=opts.verbose)\n",
      "else:\n",
      "    km = KMeans(n_clusters=true_k, init='k-means++', max_iter=100, n_init=1,\n",
      "                verbose=opts.verbose)\n",
      "\n",
      "print(\"Clustering sparse data with %s\" % km)\n",
      "t0 = time()\n",
      "km.fit(X)\n",
      "print(\"done in %0.3fs\" % (time() - t0))\n",
      "print()\n",
      "\n",
      "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
      "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
      "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
      "print(\"Adjusted Rand-Index: %.3f\"\n",
      "      % metrics.adjusted_rand_score(labels, km.labels_))\n",
      "print(\"Silhouette Coefficient: %0.3f\"\n",
      "      % metrics.silhouette_score(X, km.labels_, sample_size=1000))\n",
      "\n",
      "print()\n",
      "\n",
      "if not (opts.n_components or opts.use_hashing):\n",
      "    print(\"Top terms per cluster:\")\n",
      "    order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
      "    terms = vectorizer.get_feature_names()\n",
      "    for i in range(true_k):\n",
      "        print(\"Cluster %d:\" % i, end='')\n",
      "        for ind in order_centroids[i, :10]:\n",
      "            print(' %s' % terms[ind], end='')\n",
      "        print()\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Automatically created module for IPython interactive environment\n",
        "Usage: -c [options]\n",
        "\n",
        "Options:\n",
        "  -h, --help            show this help message and exit\n",
        "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
        "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
        "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
        "  --use-hashing         Use a hashing feature vectorizer\n",
        "  --n-features=N_FEATURES\n",
        "                        Maximum number of features (dimensions) to extract\n",
        "                        from text.\n",
        "  --verbose             Print progress reports inside k-means algorithm.\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "Usage: -c [options]\n",
        "\n",
        "-c: error: no such option: -f\n"
       ]
      },
      {
       "ename": "SystemExit",
       "evalue": "2",
       "output_type": "pyerr",
       "traceback": [
        "An exception has occurred, use %tb to see the full traceback.\n",
        "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stderr",
       "text": [
        "To exit: use 'exit', 'quit', or Ctrl-D.\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.jet() # set the color map. When your colors are lost, re-run this.\n",
      "import sklearn.datasets as datasets\n",
      "X, Y = datasets.make_blobs(centers=4, cluster_std=0.5, random_state=0)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "plt.scatter(X[:,0], X[:,1]);"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "\n",
      "plt.scatter(X[:,0], X[:,1], c=Y);\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      " X = np.random.randint(5, size=(6, 100))\n",
      " y = np.array([1, 2, 3, 4, 5, 6])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "IndentationError",
       "evalue": "unexpected indent (<ipython-input-9-06cad03e6fbc>, line 2)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-9-06cad03e6fbc>\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    X = np.random.randint(5, size=(6, 100))\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
       ]
      }
     ],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X=np.random.randint(5, size=(6,100))\n",
      "y=np.array([1,2,3,4,5,6])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X=np.random.randint(5, size=(6,100))\n",
      "y=np.array([1,2,3,4,5,6])\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-11-9a9f73266cf9>, line 4)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-11-9a9f73266cf9>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print y\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "X = np.random.randint(5, size=(6,100))\n",
      "y = np.array([1,2,3,4,5,6])\n",
      "print y"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-12-f50deafdc489>, line 4)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-12-f50deafdc489>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print y\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "lst = [10, 20, 30, 40]\n",
      "arr = np.array([10, 20, 30, 40])\n",
      "print lst\n",
      "print arr"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "SyntaxError",
       "evalue": "invalid syntax (<ipython-input-13-6e1138b2e5b7>, line 4)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-13-6e1138b2e5b7>\"\u001b[1;36m, line \u001b[1;32m4\u001b[0m\n\u001b[1;33m    print lst\u001b[0m\n\u001b[1;37m            ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
       ]
      }
     ],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from nltk.corpus import wordnet as wn\n",
      "#wn.synsets('dog')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "LookupError",
       "evalue": "\n**********************************************************************\n  Resource u'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/swathi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-25-09296a062901>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwordnet\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mwn\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msynsets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'dog'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m/home/swathi/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, attr)\u001b[0m\n\u001b[0;32m     97\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"LazyCorpusLoader object has no attribute '__bases__'\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 99\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__load\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    100\u001b[0m         \u001b[1;31m# This looks circular, but its not, since __load() changes our\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    101\u001b[0m         \u001b[1;31m# __class__ to something new:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/swathi/anaconda/lib/python2.7/site-packages/nltk/corpus/util.pyc\u001b[0m in \u001b[0;36m__load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mroot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnltk\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'corpora/%s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mzip_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m                 \u001b[1;32mexcept\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m         \u001b[1;31m# Load the corpus.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource u'corpora/wordnet' not found.  Please use the NLTK\n  Downloader to obtain the resource:  >>> nltk.download()\n  Searched in:\n    - '/home/swathi/nltk_data'\n    - '/usr/share/nltk_data'\n    - '/usr/local/share/nltk_data'\n    - '/usr/lib/nltk_data'\n    - '/usr/local/lib/nltk_data'\n**********************************************************************"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "lst = [[10, 20,30,1],[30,40,5,7],[2,4,5,8],[1,2,3,4]]\n",
      "#X = np.array([10, 20, 30, 40])\n",
      "\n",
      "km = KMeans(n_clusters = 2)\n",
      "km.fit(lst)\n",
      "\n",
      "\n",
      "centers = km.cluster_centers_\n",
      "centers[centers<0] = 0 #the minimization function may find very small negative numbers, we threshold them to 0\n",
      "#centers = centers.round(2)\n",
      "print('\\n--------Centers of the 2 different clusters--------')\n",
      "print('Deal\\t Cent1\\t Cent2')\n",
      "for i in range(1):\n",
      "    print(i+1,'\\t',centers[0,i],'\\t',centers[1,i])\n",
      "    \n",
      "prediction = km.predict(lst)\n",
      "print('\\n--------Which cluster each customer is in--------')\n",
      "print('{:<15}\\t{}'.format('Customer','Cluster'))\n",
      "for i in range(len(prediction)):\n",
      "    print('{:<15}\\t{}'.format(i,prediction[i]+1))\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "--------Centers of the 2 different clusters--------\n",
        "Deal\t Cent1\t Cent2\n",
        "(1, '\\t', 30.0, '\\t', 4.333333333333333)\n",
        "\n",
        "--------Which cluster each customer is in--------\n",
        "Customer       \tCluster\n",
        "0              \t2\n",
        "1              \t1\n",
        "2              \t2\n",
        "3              \t2\n"
       ]
      }
     ],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "lst = [[10, 20,],[30,40]]\n",
      "#arr = np.array([10, 20, 30, 40])\n",
      "print (lst)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[10, 20], [30, 40]]\n"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import numpy as np\n",
      "from sklearn.cluster import KMeans\n",
      "\n",
      "X = [[0,0.5623,0.2663,0.2496],[0.05623,0,0.6456,0.6179],[0.2663,0.6456,0,0.2350],[0.2496,0.6179,0.2350,0]]\n",
      "\n",
      "listofwords = ['weather', 'www', 'climate', 'temperature'];\n",
      "\n",
      "\n",
      "\n",
      "km = KMeans(n_clusters = 2)\n",
      "km.fit(X)\n",
      "    \n",
      "prediction = km.predict(X)\n",
      "\n",
      "print('{:<15}\\t{}'.format('Word','Cluster'))\n",
      "for i in range(len(prediction)):\n",
      "    print('{:<15}\\t{}'.format(listofwords[i],prediction[i]+1))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Word           \tCluster\n",
        "weather        \t2\n",
        "www            \t1\n",
        "climate        \t2\n",
        "temperature    \t2\n"
       ]
      }
     ],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "X=[[0,1,10,1],[1,0,10,1],[10,10,0,10],[1,1,10,0]]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    }
   ],
   "metadata": {}
  }
 ]
}